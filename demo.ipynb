{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.16.2\n",
      "troch version: 1.7.0+cu92\n"
     ]
    }
   ],
   "source": [
    "# Load Libs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nde_flows\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "print('numpy version:', np.__version__)\n",
    "print('troch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic init\n",
    "input_dim = 15 #nparams\n",
    "context_dim = 400 # Nrb * len(detectors) * 2 # for RB\n",
    "\n",
    "data_dir = 'data/'\n",
    "model_dir = 'models/'\n",
    "\n",
    "cuda = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "num_flow_steps = 15\n",
    "base_transform_kwargs={\n",
    "    'hidden_dim': 15,\n",
    "    'num_transform_blocks': 10,\n",
    "    'activation': 'elu',\n",
    "    'dropout_probability': 0.0,\n",
    "    'batch_norm': True,\n",
    "    'num_bins': 8,\n",
    "    'tail_bound': 1.0,\n",
    "    'apply_unconditional_transform': False,\n",
    "    'base_transform_type': 'rq-coupling'\n",
    "} \n",
    "\n",
    "# Train init\n",
    "lr = 0.0002\n",
    "lr_annealing = True\n",
    "anneal_method = lr_anneal_method = 'cosine'\n",
    "total_epochs = epochs = 500\n",
    "steplr_step_size = 80\n",
    "steplr_gamma = 0.5\n",
    "flow_lr = None\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 16\n",
    "output_freq = 50\n",
    "snr_annealing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the [nflows](https://github.com/bayesiains/nflows) package to implement the normalizing flow, which we take to be a [neural spline](https://arxiv.org/abs/2002.03712) flow. It makes use of the [PyTorch](https://pytorch.org) machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the neural network model.\n",
    "model_creator = nde_flows.create_NDE_model\n",
    "\n",
    "model = model_creator(input_dim=input_dim,\n",
    "                      context_dim=context_dim,\n",
    "                      num_flow_steps=num_flow_steps, \n",
    "                      base_transform_kwargs=base_transform_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Initial learning rate 0.0002\n",
      "Using cosine LR annealing.\n",
      "\n",
      "Training for 500 epochs\n"
     ]
    }
   ],
   "source": [
    "if cuda and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device', device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print('\\nInitial learning rate', lr)\n",
    "print('Using cosine LR annealing.')\n",
    "print('\\nTraining for {} epochs'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model hyperparameters:\n",
      "input_dim \t 15\n",
      "num_flow_steps \t 15\n",
      "context_dim \t 400\n",
      "base_transform_kwargs\n",
      "\t hidden_dim \t 15\n",
      "\t num_transform_blocks \t 10\n",
      "\t activation \t elu\n",
      "\t dropout_probability \t 0.0\n",
      "\t batch_norm \t True\n",
      "\t num_bins \t 8\n",
      "\t tail_bound \t 1.0\n",
      "\t apply_unconditional_transform \t False\n",
      "\t base_transform_type \t rq-coupling\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and scheduler.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_epochs,\n",
    ")\n",
    "print('\\nModel hyperparameters:')\n",
    "for key, value in model.model_hyperparams.items():\n",
    "    if type(value) == dict:\n",
    "        print(key)\n",
    "        for k, v in value.items():\n",
    "            print('\\t', k, '\\t', v)\n",
    "    else:\n",
    "        print(key, '\\t', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Wrapper.\n",
    "# (demo dataset)\n",
    "class WaveformDatasetTorch(Dataset):\n",
    "    \"\"\"Wrapper for a WaveformDataset to use with PyTorch DataLoader.\"\"\"\n",
    "    def __init__(self, data_dir, istrain):\n",
    "        self.y = torch.from_numpy(np.load(data_dir+'dataset_{}_y.npy'.format('train' if istrain else 'test'))) # 使用numpy读取数据\n",
    "        self.x = torch.from_numpy(np.load(data_dir+'dataset_{}_x.npy'.format('train' if istrain else 'test')))\n",
    "        self.w = torch.from_numpy(np.load(data_dir+'dataset_{}_w.npy'.format('train' if istrain else 'test')))\n",
    "        self.snr = torch.from_numpy(np.load(data_dir+'dataset_{}_snr.npy'.format('train' if istrain else 'test')))\n",
    "        \n",
    "        self.len = self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.y[index], self.x[index], self.w[index], self.snr[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch wrappers\n",
    "wfd_train = WaveformDatasetTorch(data_dir, istrain=True)\n",
    "wfd_test = WaveformDatasetTorch(data_dir, istrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader objects\n",
    "train_loader = DataLoader(\n",
    "    wfd_train, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=lambda _: np.random.seed(\n",
    "        int(torch.initial_seed()) % (2**32-1)))\n",
    "test_loader = DataLoader(\n",
    "    wfd_test, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=lambda _: np.random.seed(\n",
    "        int(torch.initial_seed()) % (2**32-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Train the network.\n",
    "\n",
    "  - This trains a neural conditional density estimator with a neural spline coupling flow.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting timer\n",
      "Learning rate: 0.0002\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 24.9883\n",
      "Train Epoch: 1 [800/2000 (40%)]\tLoss: 24.8968\n",
      "Train Epoch: 1 [1600/2000 (80%)]\tLoss: 23.2689\n",
      "Train Epoch: 1 \tAverage Loss: 389.6537\n",
      "Test set: Average loss: 386.1097\n",
      "\n",
      "Stopping timer.\n",
      "Training time (including validation): 107.74739027023315 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_history = []\n",
    "test_history = []\n",
    "extrinsic_at_train = True\n",
    "add_noise = False\n",
    "\n",
    "print('Starting timer')\n",
    "start_time = time.time()\n",
    "\n",
    "epoch = 1\n",
    "for epoch in range(epoch, epoch + epochs):\n",
    "    print('Learning rate: {}'.format(\n",
    "        optimizer.state_dict()['param_groups'][0]['lr']))    \n",
    "    \n",
    "    # Compute the loss\n",
    "    train_loss = nde_flows.train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        device,\n",
    "        output_freq,\n",
    "        add_noise,\n",
    "        snr_annealing)\n",
    "    test_loss = nde_flows.test_epoch(\n",
    "        model,\n",
    "        test_loader,\n",
    "        epoch,\n",
    "        device,\n",
    "        add_noise,\n",
    "        snr_annealing)\n",
    "    \n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch += 1\n",
    "    train_history.append(train_loss)\n",
    "    test_history.append(test_loss)\n",
    "\n",
    "    # Log the history to file\n",
    "    if model_dir is not None:\n",
    "        p = Path(model_dir)\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Make column headers if this is the first epoch\n",
    "        if epoch == 1:\n",
    "            with open(p / 'history.txt', 'w') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                writer.writerow([epoch, train_loss, test_loss])\n",
    "        else:\n",
    "            with open(p / 'history.txt', 'a') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                writer.writerow([epoch, train_loss, test_loss])\n",
    "    break\n",
    "\n",
    "print('Stopping timer.')\n",
    "stop_time = time.time()\n",
    "print('Training time (including validation): {} seconds'\n",
    "      .format(stop_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save a model and optimizer to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "p = Path(model_dir)\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dict = {\n",
    "    'model_hyperparams': model.model_hyperparams,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "}\n",
    "\n",
    "if scheduler is not None:\n",
    "    dict['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "torch.save(dict, p / 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
